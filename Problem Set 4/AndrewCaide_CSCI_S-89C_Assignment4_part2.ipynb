{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Andrew Caide\n",
    "### CSCI S-89C Deep Reinforcement Learning  \n",
    "### Part II of Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider Environment that has five states: 1, 2, 3, 4, and 5. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; (3) 3->2, 3->3, 3->4; (4) 4->3, 4->4, 4->5; (5) 5->4, 5->5.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and +1 action will result in staying - in other words, there is an \"east wind.\" More specifically, the non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "Further, we assume that whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "\n",
    "\n",
    "Further, assume that the agent does not know about the wind or what rewards to expect. It chooses to stay in all states, i.e. the policy is\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=1, \\pi(+1|1)=0$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=1, \\pi(+1|2)=0$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=0, \\pi(0|4)=1, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "Please estimate the action-value function using First-Visit Monte Carlo (MC) prediction. Make sure each pair $(S_0,A_0)$ has non-zero probability of being selected. Let‚Äôs use $\\gamma=0.9$ and run the episodes for $T=100$. Make some reasonable selection of tolerance $\\theta$.\n",
    "\n",
    "Based on the estimates of $q_\\pi(s,a)$, please suggest a better policy. Is it optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Setting up Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1) \n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def set_state(self, new_state):\n",
    "        self.state = new_state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        # If (we're not in s1 and moving to the right) or (if we're on ther right and move == 1)\n",
    "        if (self.state > 1 and move > -1) or (self.state == 1 and move > 0):\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        \n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "#########\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "        self.scan = 1\n",
    "        self.scan_done = False\n",
    "\n",
    "    def step(self, env, take_action = False, policy = 1):\n",
    "        \n",
    "        # If taking action: policy 1 = random movement, policy 2 = converge to S3\n",
    "        if take_action:\n",
    "            action_selected = random.choice(env.admissible_actions())\n",
    "        else:\n",
    "            action_selected = 0 # Stay put\n",
    "        \n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please estimate the action-value function using First-Visit Monte Carlo (MC) prediction. Make sure each pair  (ùëÜ0,ùê¥0)  has non-zero probability of being selected. \n",
    "1. Let‚Äôs use  ùõæ=0.9.\n",
    "2. Run the episodes for  ùëá=100. \n",
    "3. Make some reasonable selection of tolerance  ùúÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_trial(theta=0.1, State=1, gamma=0.9, act = False, policy = 1):\n",
    "    \n",
    "    # Initial state chosen randomly\n",
    "    S0 = random.randint(1,5)\n",
    "    \n",
    "    # Set up Env and conditions\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    G_old = 0\n",
    "    delta = 1\n",
    "    record_rewards = False\n",
    "    i = 0\n",
    "    \n",
    "    while delta > theta:\n",
    "        \n",
    "        agent.step(env, act, policy)\n",
    "        if env.check_state() == State:  # Once we hit St = t, we start recording rewards \n",
    "            record_rewards = True\n",
    "            \n",
    "        if record_rewards:\n",
    "            G += G*gamma**(env.time-1) + agent.current_reward\n",
    "            if i == 0:\n",
    "                G_old = G\n",
    "                i += 1\n",
    "            else:\n",
    "                delta = G - G_old\n",
    "                G_old = G\n",
    "                i += 1\n",
    "\n",
    "        # We need to perform a series of checks if we expect the agent to stand still\n",
    "        if not act:\n",
    "            # If the current state is less than 3 and we have not hit the reward (we won't!) return 0\n",
    "            if env.check_state() < 3 and not record_rewards:\n",
    "                return G\n",
    "\n",
    "            # If we under-shoot our starting-point S0, return NA - we can't record rewards\n",
    "            if State > S0 and not record_rewards:\n",
    "                return np.nan\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fv_action_estimator(theta, State, gamma, episodes = 100, act = False, policy = 1):\n",
    "    S = []\n",
    "    for i in range(episodes):\n",
    "        S.append(first_visit_trial(theta,State,gamma, act, policy))\n",
    "    return round(np.nanmean(S),3) # We're returning nan's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V(St=1) = 0.0', 'V(St=2) = 0.0', 'V(St=3) = 864.07', 'V(St=4) = 78.999', 'V(St=5) = 0.0']\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "V = np.array([fv_action_estimator(theta=.001, State = s, gamma=0.9, episodes=100) for s in range(1,6)])\n",
    "print([\"V(St={}) = {}\".format(i+1, v) for i, v in enumerate(V)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Based on the estimates of  ùëûùúã(ùë†,ùëé) , please suggest a better policy. Is it optimal?\n",
    "\n",
    "Testing two policies: \n",
    "1. Equal chance to do any of the admissible actions\n",
    "2. Always converge to $S_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V(St=1) = 0.0', 'V(St=2) = 134.01', 'V(St=3) = 872.777', 'V(St=4) = 151.546', 'V(St=5) = 0.0']\n"
     ]
    }
   ],
   "source": [
    "# 1 - Random actions:\n",
    "T = 100\n",
    "V = np.array([fv_action_estimator(theta=.001, State = s, gamma=0.9, episodes=100, act = True) for s in range(1,6)])\n",
    "print([\"V(St={}) = {}\".format(i+1, v) for i, v in enumerate(V)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Policy 2 (listed above) doesn't work; First Visit MC requires the agent to touch the state we're measuring the state-value for before we start adding rewards. In the case of always-converge, there is a strong chance, when randomly placed in the environment, that the agent will **miss** the target state. \n",
    "\n",
    "Example: Target = 2 & S0 = 4, or Target = 1, S0 = 2. In these cases, the agent will converge to S=3 and stay there, never hitting the target (and we start infinitely looping because of our exit condition - the delta - is never satisfied). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Is it optimal**?\n",
    "\n",
    "Definitely not, but I think it's *better*, by randomly acting around, the agent has a chance to explore the environment a little better. Under this situation, $V_{St}(2 and 4)$ see some rewards. Previously, $V_{St}(4)$ was seeing some rewards but it was purely due to drift. In the random walk condition, it sees more rewards but as does $V_{St}(2)$, and this can be useful in a situation where the rewards weren't solely centralized in $V_{St}(3)$ \n",
    "\n",
    "At the end of the simulations, $V_{St}(3)$ nets the most rewards by almost 4x, indicating that it's obviously the optimal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, the agent will obtain the optimal policy via MC control. Please run the On-policy first-visit MC control (for $\\varepsilon$-soft policies)  without Exploring Starts algorithm for estimating $\\pi_*$. Use same $\\gamma=0.9$ and $T=100$.\n",
    "\n",
    "Does the final policy appear to be optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating agent to use a policy\n",
    "import pandas as pd\n",
    "\n",
    "class Policy_Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "        self.scan = 1\n",
    "        self.scan_done = False\n",
    "        self.action_selected = 0\n",
    "        \n",
    "        ## Help! There has to be a smarter way of doing this. \n",
    "        self.policy = {1:[0, 1/2, 1/2], 2: [1/3,1/3,1/3],\n",
    "                      3: [1/3,1/3,1/3], 4: [1/3,1/3,1/3],\n",
    "                      5: [1/2,1/2,0]}\n",
    "\n",
    "    def update_policy(self, new_policy):\n",
    "        self.policy = new_policy\n",
    "\n",
    "    def step(self, env):\n",
    "        \n",
    "        self.action_selected = np.random.choice([-1,0,1], 1, p=self.policy[env.check_state()])[0]\n",
    "        reward = env.get_reward(self.action_selected)  \n",
    "        self.current_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Completely recycled; updating to kleep track of R(S, A)\n",
    "\n",
    "# This implementation is questionable. \n",
    "\n",
    "def first_visit_trial(theta=0.1, State=1, gamma=0.9, new_policy = False):\n",
    "    \n",
    "    # Initial state chosen randomly\n",
    "    S0 = random.randint(1,5)\n",
    "    \n",
    "    # Set up Env and conditions\n",
    "    env = Environment(S0)\n",
    "    agent = Policy_Agent()\n",
    "    if new_policy:\n",
    "        agent.update_policy(new_policy) \n",
    "    R = 0\n",
    "    R_old = 0\n",
    "    delta = 10\n",
    "    record_rewards = False\n",
    "    i = 0\n",
    "    rewards = {1:{-1:0,0:0,1:0}, 2:{-1:0,0:0,1:0}, 3:{-1:0,0:0,1:0},\n",
    "               4:{-1:0,0:0,1:0}, 5:{-1:0,0:0,1:0}}\n",
    "    \n",
    "    while delta > theta:\n",
    "        \n",
    "        old_state = env.check_state()\n",
    "        agent.step(env)\n",
    "        action = agent.action_selected\n",
    "        \n",
    "        if env.check_state() == State:  # Once we hit St = t, we start recording rewards \n",
    "            record_rewards = True\n",
    "            \n",
    "        if record_rewards:\n",
    "            R += R*gamma**(env.time-1) + agent.current_reward\n",
    "            if i == 0:\n",
    "                R_old = R\n",
    "                i += 1\n",
    "            else:\n",
    "                delta = R - R_old\n",
    "                R_old = R\n",
    "                i += 1\n",
    "                rewards[old_state][action] = round(R,4)\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall recycle first_visit_trial function; it still can serve a purpose\n",
    "# Need to rework the fv_action_estimator; we want a policy estimator!\n",
    "\n",
    "def FVMC_policy_estimator(theta=1, State=3, gamma=.9, episodes = 100, eps = .3, new_policy = False):\n",
    "    # There REALLY has to be a smarter way of doing this...\n",
    "    \n",
    "    s1 = pd.DataFrame()\n",
    "    s2 = pd.DataFrame()\n",
    "    s3 = pd.DataFrame()\n",
    "    s4 = pd.DataFrame()\n",
    "    s5 = pd.DataFrame()\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        if new_policy:\n",
    "            results = first_visit_trial(theta,State,gamma, new_policy)\n",
    "        else:\n",
    "            results = first_visit_trial(theta,State,gamma)\n",
    "        \n",
    "        s1 = s1.append(results[1], ignore_index = True)\n",
    "        s2 = s2.append(results[2], ignore_index = True)\n",
    "        s3 = s3.append(results[3], ignore_index = True)\n",
    "        s4 = s4.append(results[4], ignore_index = True)\n",
    "        s5 = s5.append(results[5], ignore_index = True)\n",
    "    \n",
    "    results = pd.DataFrame({1: s1.mean(), 2: s2.mean(), 3:s3.mean(), 4:s4.mean(),5:s5.mean()})\n",
    "    records = results.copy() #for closer examination\n",
    "    \n",
    "    # Get new policy\n",
    "    for col in results.columns:\n",
    "        A = results[col] == results[col].max()\n",
    "        if col in [2, 3, 4]:\n",
    "            results.loc[A,col] = 1-eps+eps/3\n",
    "            results.loc[-A,col]= eps/3\n",
    "        else:\n",
    "            not0 = (results[col] != 0) & (-A)\n",
    "            results.loc[A,col] = 1-eps+eps/2\n",
    "            results.loc[not0,col] = eps/2\n",
    "    return ([records, results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Reward values:\n",
      "             1           2           3           4           5\n",
      "-1    0.000000  400.224339  409.593904  380.926835  280.243363\n",
      " 0  403.544919  391.399529  361.401660  262.717017  228.231358\n",
      " 1  405.140990  413.127591  374.290122  302.952976    0.000000\n",
      "\n",
      "Updated policies:\n",
      "       1    2    3    4     5\n",
      "-1  0.00  0.1  0.8  0.8  0.85\n",
      " 0  0.15  0.1  0.1  0.1  0.15\n",
      " 1  0.85  0.8  0.1  0.1  0.00\n"
     ]
    }
   ],
   "source": [
    "r = FVMC_policy_estimator(State=3)\n",
    "print(\"-------------------\\nReward values:\")\n",
    "print(r[0])\n",
    "print(\"\\nUpdated policies:\")\n",
    "print(r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are *almost* optimal; The agent clearly wants to move to position 3, but depending on RNG (not sure what's going on here), at state 3 it will want to move either left or right, but immediately return back to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus, update agent with new policy and try again.\n",
    "\n",
    "new_policy = {}\n",
    "for_updating = r[1].to_dict().items()\n",
    "for key, value in for_updating:\n",
    "    new_policy[key] = list(value.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Reward values:\n",
      "             1           2           3           4          5\n",
      "-1    0.000000  126.518380  139.842284  132.209775  44.417125\n",
      " 0   45.601373  126.695266  118.962205   44.028881   0.000000\n",
      " 1  127.900760  138.326346  130.958482   44.066135   0.000000\n",
      "\n",
      "Updated policies:\n",
      "       1    2    3    4     5\n",
      "-1  0.00  0.1  0.8  0.8  0.85\n",
      " 0  0.15  0.1  0.1  0.1  0.00\n",
      " 1  0.85  0.8  0.1  0.1  0.00\n"
     ]
    }
   ],
   "source": [
    "r = FVMC_policy_estimator(State=3, new_policy = new_policy)\n",
    "print(\"-------------------\\nReward values:\")\n",
    "print(r[0])\n",
    "print(\"\\nUpdated policies:\")\n",
    "print(r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't say for sure it's an improvement, but it's definitely converging faster around 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
