{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Andrew Caide\n",
    "### CSCI S-89C Deep Reinforcement Learning  \n",
    "### Part II of Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we consider patients with end-stage liver disease (ESLD). We assume that patient's health condition is fully characterized by the Model for End-stage Liver Disease (MELD) score (Jae-Hyeon Ahn and John Hornberger, Involving patients in the cadaveric kidney transplant allocation process: a decision-theoretic perspective. Manage Sci. 1996;42(5):629â€“41).\n",
    "\n",
    "The MELD score ranges from 6 to 40 and is derived based on the probability of survival at 3 months for patients with ESLD. Data in ESLD is usually sparse and often aggregated into Stages. We assume that there are 18 stages based on the ESLD: Stage 1, Stage2, ..., Stage 18. The time step is 1 year and the actions in Stages 1 through 18 are \"wait\" (denoted by 0) and \"transplant\" (denoted by 1). \n",
    "\n",
    "We assume that the Markov property holds. There are two additional states of the Markov Decision Process: \"Posttransplant Life\" (denoted by 19) and \"Death\" (which is denoted by 20 and combines so caled \"Pretransplant Death\" and \"Posttransplant Death\"). The only action availible in state \"Posttransplant Life\" is \"wait\" and \"Death\" is the terminal state with no actions. Assume that the length of an episode is T=50, unless it terminates earlier due to the transition to the absorbing state \"Death.\"\n",
    "\n",
    "We do not know the transition probabilities, but if a patient selects \"wait,\" the possible transitions are   \n",
    "1) Stage 1->Stage 1, Stage 1->Stage 2, Stage 1->Death  \n",
    "2) For k in {2,3,4,...17}, Stage k->Stage (k-1), Stage k->Stage k, Stage k->Stage (k+1), Stage k->Death    \n",
    "3) Stage 18->Stage 17, Stage 18->Stage 18, Stage 18->Death    \n",
    "\n",
    "If a patient selects \"transplant\" at Stage k, k=1,2,...,18, the only possible transition is  \n",
    "4) Stage k->\"Posttransplant Life\"\n",
    "\n",
    "Finally, there are two more possible transitions\"  \n",
    "5) \"Posttransplant Life\"->\"Posttransplant Life\" and \"Posttransplant Life\"->\"Death\"  \n",
    "\n",
    "\n",
    "The patient gets reward 1 in all states \"Stage k\" (k=1,2,...,18) and reward 0.2 in the \"Posttransplant Life\" state - assume that the patient gets these rewards on \"exit\" from the states, i.e. after we observe the corresponding stage. We assume the discounting parameter $\\gamma=0.97$, one of the most common discounting rate used in medical decision making (Gold MR, Siegel JE, Russell LB, Weinstein MC. Cost-Effectiveness in Health and Medicine. Oxford University Press; New York: 1996).\n",
    "\n",
    "\n",
    "Please consider statistics on 8,000 patients with ESLD saved in the 'ESLD_statistics.csv' file. Each row represents an episode (i.e. one patient) and the columns are the sequences of the patients' states and actions. This data were generated under the behavor policy:\n",
    "\n",
    "$b(1|k)=0.02$ for $k\\in\\{1,2,3,4,5,6,7,8,9,10,11,12,13\\}$;   \n",
    "$b(1|14)=0.05$;   \n",
    "$b(1|15)=0.10$;   \n",
    "$b(1|16)=0.20$;   \n",
    "$b(1|17)=0.40$;  \n",
    "$b(1|18)=0.60$;  \n",
    "\n",
    "which means that, for example, 5% of paients at stage 14 received a transplant.\n",
    "\n",
    "---\n",
    "---\n",
    "## TASK:\n",
    "\n",
    "Please use the Off policy MC control (for estimating $\\pi_*$), which corresponds to the weighted importance sampling, to obtain the optimal policy. Please be specific and **answer at what stages it is worth considering a transplant and at which stages - not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ESLD_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "#### Discount:    \n",
    "$\\gamma = 0.97$\n",
    "\n",
    "#### Actions:\n",
    "$A \\in\\{0:Wait, 1:Transplant\\} $;    \n",
    "\n",
    "#### Rewards:    \n",
    "$R(A|S_{\\in\\{1,...,18\\}}) = 1$    \n",
    "$R(A|S_{19}) = 0.2$    \n",
    "$R(A|S_{20}) = 0$\n",
    "\n",
    "\n",
    "#### Behavior Policies:    \n",
    "$b(1|k)=0.02$ for $k\\in\\{1,2,3,4,5,6,7,8,9,10,11,12,13\\}$;   \n",
    "$b(1|14)=0.05$;   \n",
    "$b(1|15)=0.10$;   \n",
    "$b(1|16)=0.20$;   \n",
    "$b(1|17)=0.40$;  \n",
    "$b(1|18)=0.60$;  \n",
    "\n",
    "---\n",
    "#### Possible Transition States:     \n",
    "$S_{T+1}(0|S_{1}) = S\\in\\{S_{1}, S_{2}, Death\\}$;  \n",
    "$S_{T+1}(0|S_{\\in\\{2,...,17\\}}) = S\\in\\{S_{T-1}, S_{T},S_{T+1}, Death\\}$;  \n",
    "$S_{T+1}(0|S_{18}) = S\\in\\{S_{17}, S_{18}, Death\\}$; \n",
    "\n",
    "**If there's a transplant**    \n",
    "$S_{T+1}(1|S_{T}) = S_{PTL} = S_{19}$; \n",
    "\n",
    "Note Important States:    \n",
    "$S_{19}:$ Post-Transplant Life   \n",
    "$S_{20}:$ Death\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(n):\n",
    "    # n == state(t+1)\n",
    "    if n < 19:\n",
    "        return 1\n",
    "    elif n == 19:\n",
    "        return .2\n",
    "    else: return 0\n",
    "    \n",
    "def behavior_policy_from_assignment(state, action):\n",
    "    # State: 1-20, action: 0-1\n",
    "    behaviors = {13:.02, 14:.05, 15:.1, 16:.2, 17:.4, 18:.6}\n",
    "    \n",
    "    if state <= 13:\n",
    "        behavior = [1-behaviors[13], behaviors[13]]\n",
    "    elif state > 13 and state < 19:\n",
    "        behavior = [1-behaviors[state], behaviors[state]]\n",
    "    elif state >= 19:\n",
    "        behavior = [1, 0] # Once in states 19, 20, we always expect Action to be 0. \n",
    "    return behavior[action]\n",
    "\n",
    "def e_soft_policy(state, action, eta = 0.7):\n",
    "    # Default to staying at 0\n",
    "    return([1-eta+eta/2, eta/2][action])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offpolicy_MC_control(data, behavior_policy):\n",
    "    \n",
    "    gamma = 0.97\n",
    "    # Initialize for all S, A\n",
    "    Q = pd.DataFrame(0, index=range(1,21), columns={0,1})\n",
    "    C = pd.DataFrame(0, index=range(1,21), columns={0,1})\n",
    "    target_policy = Q.idxmax(1) # Consistency Rule: First max value\n",
    "    \n",
    "    for ep in range(len(data)):\n",
    "        episode = data.loc[ep,:]\n",
    "        G = 0 \n",
    "        W = 1\n",
    "         \n",
    "        for step in range(0, 50):\n",
    "            # Organize states\n",
    "            st = episode[step*2]\n",
    "            st1= episode[step*2+2]\n",
    "            at = episode[step*2+1]\n",
    "            \n",
    "            G = gamma*G + get_reward(st1)\n",
    "            C.loc[st,at] = C.loc[st,at] + W\n",
    "            Q.loc[st,at] = Q.loc[st,at] + W/C.loc[st,at]*(G - Q.loc[st,at])\n",
    "            target_policy[st] = Q.loc[st,:].idxmax(axis=1)\n",
    "            \n",
    "            if at != target_policy[st]:\n",
    "                break\n",
    "            W = W/behavior_policy(st, at)\n",
    "            #W = W/get_behavior_policy(st, at)\n",
    "    return [target_policy, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "esoft = offpolicy_MC_control(data, behavior_policy = e_soft_policy)\n",
    "provided_policy = offpolicy_MC_control(data, behavior_policy = behavior_policy_from_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Using an e-soft $b(S,A)$ policy, the following five states have been identified as important windows to providing the patient with a transplant: $1, 13, 14, 15, 17$. More data about the policy choices and Action-State values can be found in the esoft variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 13, 14, 15, 17]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esoft[0][esoft[0]==1].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosoity, I also tried implementing the provided behavior policy to the off-policy MDP as a behavior policy. I wasn't sure exactly why the purpose of having been provided this policy was, so I understood it as the policy we had to use. \n",
    "\n",
    "In any case, the following states have been identified as critical: $1, 3, 6, 9, 10, 14$. Interestingly, states $1$ and $14$ have been identified as important by both policies. These two states, $1$ and $14$, are definitely important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 9, 10, 14]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provided_policy[0][provided_policy[0]==1].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states below have been identified as less worth considering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 16, 18, 19, 20]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esoft[0][esoft[0]==0].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
