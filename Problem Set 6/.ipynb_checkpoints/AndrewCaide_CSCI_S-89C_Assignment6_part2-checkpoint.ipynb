{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Andrew Caide\n",
    "### CSCI S-89C Deep Reinforcement Learning      \n",
    "### Part II of Assignment 6      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider Environment that has five states: 1, 2, 3, 4, and 5. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; (3) 3->2, 3->3, 3->4; (4) 4->3, 4->4, 4->5; (5) 5->4, 5->5.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and +1 action will result in staying - in other words, there is an \"east wind.\" More specifically, the non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "Further, we assume that whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "\n",
    "\n",
    "Further, assume that the agent does not know about the wind or what rewards to expect. It chooses to stay in all states, i.e. the policy is\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=1, \\pi(+1|1)=0$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=1, \\pi(+1|2)=0$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=0, \\pi(0|4)=1, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "# GOAL:\n",
    "Please estimate the state-value function using one-step Temporal Difference (TD) prediction. Letâ€™s use $\\gamma=0.9$ and run the episodes for $T=100$.\n",
    "\n",
    "---\n",
    "\n",
    "### States:\n",
    "$S \\in\\{S_{1},S_{2},S_{3},S_{4},S_{5} \\} $;    \n",
    "\n",
    "\n",
    "\n",
    "#### Actions:\n",
    "$A \\in\\{-1, 0, 1\\} if S  \\in\\{S_{2},S_{3},S_{4}\\}$;     \n",
    "$A \\in\\{0, 1\\} if S == S_{1}$;  \n",
    "$A \\in\\{-1, 0\\} if S == S_{5}$;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1) \n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def set_state(self, new_state):\n",
    "        self.state = new_state\n",
    "        \n",
    "    def get_stateValue(state):\n",
    "        statValues = {1:0, 2:0, 3:1, 4:0, 5:1}\n",
    "        return stateValues[state]\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        # If (we're not in s1 and moving to the right) or (if we're on the right and move == 1)\n",
    "        if (self.state > 1 and move > -1) or (self.state == 1 and move > 0):\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        \n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "        \n",
    "#########\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env, take_action = False, policy = 1):\n",
    "        \n",
    "        # If taking action: policy 1 = random movement, policy 2 = converge to S3\n",
    "        if take_action:\n",
    "            action_selected = random.choice(env.admissible_actions())\n",
    "        else:\n",
    "            action_selected = 0 # Stay put\n",
    "        \n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_td(n=1, State0=1, gamma=0.9, alpha = 0.1, act = False, episodes = 100):\n",
    "    \n",
    "    if not State0:\n",
    "        S0 = random.randint(2,5)\n",
    "    \n",
    "    # Initialize V(s) arbitrarily for all S\n",
    "    V = pd.DataFrame(0, index=range(1,6), columns={\"V\"})\n",
    "    \n",
    "\n",
    "    # Loop for each episode\n",
    "    for i in range(episodes):\n",
    "        \n",
    "        # Set up Env and conditions\n",
    "        env = Environment(State0)\n",
    "        agent = Agent()\n",
    "        T = float('inf')\n",
    "        t=0\n",
    "        states = [State0]\n",
    "        rewards = [0]\n",
    "        \n",
    "        while True:\n",
    "            t +=1\n",
    "            if t<T:\n",
    "                # Take an action according to policy (dont't act!)\n",
    "                agent.step(env, act)\n",
    "                # Observe and store nexrt reward and states\n",
    "                states.append(env.check_state())\n",
    "                rewards.append(agent.current_reward)\n",
    "              \n",
    "                #1 is a terminal state; we can't move once we hit S1\n",
    "                if env.check_state() == 1: \n",
    "                    T = t + 1\n",
    "                \n",
    "            tao = t-n+1\n",
    "            if tao>= 0:\n",
    "                returns = 0\n",
    "                # Compute G\n",
    "                for t in range(tao + 1, min(T, tao+n)):\n",
    "                    returns += gamma**(t-tao-i) * rewards[t]\n",
    "                if tao + n-1 < T:\n",
    "                    # G <- G + gamma**V(S_{tao+n})\n",
    "                    # Having issues indexing, I think something weird is going on here.\n",
    "                    returns += gamma**n + V.loc[states[tao+n-1]].values[0]\n",
    "                \n",
    "                # Find the state to update from list, update it\n",
    "                state_to_update = states[tao-1]\n",
    "                if state_to_update != 1:\n",
    "                    V.loc[state_to_update] += alpha * (returns-V.loc[state_to_update])\n",
    "            if tao == T-1:\n",
    "                break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's take a look at the values if we set the starting point to be stage 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.581481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.357125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.395570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.941986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V\n",
       "1   0.000000\n",
       "2   2.581481\n",
       "3   8.357125\n",
       "4  13.395570\n",
       "5  15.941986"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_td(State0 = 5,n = 1, alpha = 0.5, episodes = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, I'm expecting 0's at 5 if we're doing n==1...\n",
    "\n",
    "Below is a bootstrap result from 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V    0.000000\n",
       "V    0.599145\n",
       "V    3.148424\n",
       "V    6.154287\n",
       "V    7.926816\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = [one_step_td(State0 = n,n = 1, alpha = 0.5, episodes = 100) for n in range(1,6)]\n",
    "pd.concat(V, axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, the agent will obtain the optimal policy via Double Q-learning. Please run the Double Q-learning algorithm - make sure to generate each pair $(S_t,A_t)$ using $\\varepsilon$-soft policy with respect to curret action-value function $(Q_1+Q_2)/2$. Use same $\\gamma=0.9$ and $T=100$.\n",
    "\n",
    "Does the final policy appear to be optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "        self.scan = 1\n",
    "        self.scan_done = False\n",
    "\n",
    "    def step(self, env, take_action = False, policy = 1):\n",
    "        \n",
    "        # If taking action: policy 1 = random movement, policy 2 = converge to S3\n",
    "        if take_action:\n",
    "            action_selected = take_action\n",
    "        else:\n",
    "            action_selected = 0 # Stay put\n",
    "        \n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_q(q, eps):\n",
    "    # Get the optimal policy given a specific state\n",
    "    # Identify max value, set it as the e-greedy value\n",
    "    if any(q ==0):\n",
    "        policy = pd.DataFrame(eps/2, index=range(1,2), columns={-1,0,1})\n",
    "        policy.loc[:,q.idxmax(1)] = 1-eps + eps/2\n",
    "        policy.loc[:,q.idxmin(1)] = 0\n",
    "    else:\n",
    "        policy = pd.DataFrame(eps/3, index=range(1,2), columns={-1,0,1})\n",
    "        policy.loc[:,q.idxmax(1)] = 1-eps + eps/3\n",
    "    \n",
    "    return(policy.values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q(alpha = 0.1, gamma = 0.9, S0 = 5, episodes = 100, eps = .3):\n",
    "    # initiate Q1, Q2 for all S and A, such that terminal = 0\n",
    "    Q1 = pd.DataFrame(1, index=range(1,6), columns={-1,0,1})\n",
    "    Q2 = pd.DataFrame(1, index=range(1,6), columns={-1,0,1})\n",
    "    Q1.loc[1,:] = 0\n",
    "    Q2.loc[1,:] = 0\n",
    "    Q1.loc[5,1] = 0\n",
    "    Q2.loc[5,1] = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        if not S0:\n",
    "            S0 = random.randint(2,5)\n",
    "        env = Environment(S0)\n",
    "        agent = Agent()\n",
    "        S = env.check_state()\n",
    "        \n",
    "        while True:\n",
    "            action_vals = (Q1+Q2)/2\n",
    "            Q = action_vals.loc[S]\n",
    "            \n",
    "            # Make greedy policy\n",
    "            policy = get_policy_from_q(Q, eps)\n",
    "            A = np.random.choice([0, 1,-1], 1, p=policy)[0] #This might be the problem\n",
    "            \n",
    "            # Take action A, observe R, S'\n",
    "            agent.step(env, A)\n",
    "            reward = agent.current_reward # R\n",
    "            S1 = env.check_state()        # S'\n",
    "            \n",
    "            # With a 0.5 probability:\n",
    "            if random.choice([True,False]):\n",
    "                # Argmax Q1(S',*)\n",
    "                best_q1_action = Q1.loc[S1].idxmax(1)\n",
    "                \n",
    "                Q1.loc[S,A] = Q1.loc[S,A] +\\\n",
    "                    alpha*(reward +\\\n",
    "                           gamma*Q2.loc[S1,best_q1_action] - Q1.loc[S,A])\n",
    "                \n",
    "            else:\n",
    "                # Argmax Q2(S',*)\n",
    "                best_q2_action = Q2.loc[S1].idxmax(1)\n",
    "                \n",
    "                Q2.loc[S,A] = Q2.loc[S,A] +\\\n",
    "                    alpha*(reward +\\\n",
    "                           gamma*Q1.loc[S1,best_q2_action] - Q2.loc[S,A])\n",
    "            \n",
    "            S = S1\n",
    "            # If S is terminal:\n",
    "            if S == 1:\n",
    "                break\n",
    "    Q_final = (Q1+ Q2)/2\n",
    "    return Q_final.divide(Q_final.max(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.782179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934627</td>\n",
       "      <td>0.908086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861481</td>\n",
       "      <td>0.720623</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.701426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1        -1\n",
       "1       NaN       NaN       NaN\n",
       "2  0.782179  1.000000  0.001219\n",
       "3  1.000000  0.934627  0.908086\n",
       "4  0.861481  0.720623  1.000000\n",
       "5  0.701426  0.000000  1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = double_q(S0 = False)\n",
    "qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Yes - the action-values derived from the algorithm hone-in on $S_{3}$. Highly unoptimal state-action values have been identified as Q(2,-1), Q(4, {0,1}), etc. In most cases, moving to $S_{3}$ is prefered by an order of 2-3x the other action values in their respective states.\n",
    "However, what's interesting is that $Q(3, {1,-1})$ aren't as low as I'd expect them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
